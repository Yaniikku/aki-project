{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A1FPeunAKe-y"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "import json\n",
        "import numpy as np\n",
        "from pathlib import Path\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score, confusion_matrix, classification_report\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "from torch.optim import AdamW\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tqdm import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# ‚úÖ Modellname hier anpassen\n",
        "model_name = \"distilbert-base-uncased\"  # z.B. \"roberta-base\", \"albert-base-v2\", \"prajjwal1/bert-tiny\"\n",
        "\n",
        "# ‚úÖ Device setup\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "use_amp = device.type == \"cuda\"\n",
        "\n",
        "# ‚úÖ Load and prepare data\n",
        "df = pd.read_csv(\"super_sms_dataset.csv\", encoding=\"latin1\")\n",
        "df = df.rename(columns={\"SMSes\": \"text\", \"Labels\": \"label\"})\n",
        "df = df.dropna(subset=[\"label\", \"text\"])\n",
        "df[\"label\"] = df[\"label\"].astype(int)\n",
        "\n",
        "# ‚úÖ Reduziere auf 30 % (stratifiziert)\n",
        "df_sampled = df.groupby(\"label\", group_keys=False).apply(lambda x: x.sample(frac=0.3, random_state=42))\n",
        "print(\"‚úÖ Genutzter Datensatz:\", len(df_sampled), \"Eintr√§ge\")\n",
        "\n",
        "# ‚úÖ Split\n",
        "train_texts, val_texts, train_labels, val_labels = train_test_split(\n",
        "    df_sampled[\"text\"].tolist(), df_sampled[\"label\"].tolist(), test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# ‚úÖ Tokenizer und Modell (dynamisch)\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2)\n",
        "for param in model.base_model.parameters():  # üßä Encoder optional einfrieren oder trainieren\n",
        "    param.requires_grad = True\n",
        "model.to(device)\n",
        "optimizer = AdamW(model.parameters(), lr=2e-5)\n",
        "\n",
        "# ‚úÖ Dataset\n",
        "class SpamDataset(Dataset):\n",
        "    def __init__(self, texts, labels):\n",
        "        self.encodings = tokenizer(texts, truncation=True, padding=True, max_length=64, return_tensors='pt')\n",
        "        self.labels = torch.tensor(labels)\n",
        "\n",
        "    def __len__(self): return len(self.labels)\n",
        "    def __getitem__(self, idx):\n",
        "        item = {key: val[idx] for key, val in self.encodings.items()}\n",
        "        item['labels'] = self.labels[idx]\n",
        "        return item\n",
        "\n",
        "train_dataset = SpamDataset(train_texts, train_labels)\n",
        "val_dataset = SpamDataset(val_texts, val_labels)\n",
        "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=16)\n",
        "\n",
        "# ‚úÖ Early Stopping Setup\n",
        "best_val_loss = float('inf')\n",
        "patience = 4\n",
        "counter = 0\n",
        "early_stop = False\n",
        "\n",
        "# ‚úÖ Training\n",
        "num_epochs = 9\n",
        "train_losses = []\n",
        "val_losses = []\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    print(f\"\\nEpoch {epoch + 1}/{num_epochs}\")\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "\n",
        "    for batch in tqdm(train_loader):\n",
        "        batch = {k: v.to(device) for k, v in batch.items()}\n",
        "        optimizer.zero_grad()\n",
        "        with torch.cuda.amp.autocast(enabled=use_amp):\n",
        "            outputs = model(**batch)\n",
        "            loss = outputs.loss\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    avg_train_loss = total_loss / len(train_loader)\n",
        "    train_losses.append(avg_train_loss)\n",
        "    print(f\"üîß Avg Training Loss: {avg_train_loss:.4f}\")\n",
        "\n",
        "    # ‚úÖ Validation loss\n",
        "    model.eval()\n",
        "    val_loss = 0\n",
        "    with torch.no_grad():\n",
        "        for batch in val_loader:\n",
        "            batch = {k: v.to(device) for k, v in batch.items()}\n",
        "            outputs = model(**batch)\n",
        "            val_loss += outputs.loss.item()\n",
        "\n",
        "    avg_val_loss = val_loss / len(val_loader)\n",
        "    val_losses.append(avg_val_loss)\n",
        "    print(f\"üß™ Avg Validation Loss: {avg_val_loss:.4f}\")\n",
        "\n",
        "    if avg_val_loss < best_val_loss:\n",
        "        best_val_loss = avg_val_loss\n",
        "        counter = 0\n",
        "        print(\"‚úÖ Validation loss improved.\")\n",
        "    else:\n",
        "        counter += 1\n",
        "        print(f\"‚ö†Ô∏è No improvement. Patience: {counter}/{patience}\")\n",
        "        if counter >= patience:\n",
        "            print(\"‚èπÔ∏è Early stopping triggered.\")\n",
        "            early_stop = True\n",
        "            break\n",
        "\n",
        "# ‚úÖ Lernkurve\n",
        "plt.figure(figsize=(8, 5))\n",
        "plt.plot(range(1, len(train_losses) + 1), train_losses, label=\"Train Loss\")\n",
        "plt.plot(range(1, len(val_losses) + 1), val_losses, label=\"Validation Loss\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.title(\"üìâ Lernkurve\")\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# ‚úÖ Evaluation\n",
        "model.eval()\n",
        "all_preds, all_labels = [], []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for batch in val_loader:\n",
        "        batch = {k: v.to(device) for k, v in batch.items()}\n",
        "        outputs = model(**batch)\n",
        "        preds = outputs.logits.argmax(dim=1).cpu()\n",
        "        labels = batch['labels'].cpu()\n",
        "        all_preds.extend(preds.numpy())\n",
        "        all_labels.extend(labels.numpy())\n",
        "\n",
        "print(\"\\nüìä Classification Report:\")\n",
        "print(classification_report(all_labels, all_preds))\n",
        "\n",
        "# ‚úÖ Confusion Matrix\n",
        "cm = confusion_matrix(all_labels, all_preds)\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap=\"Blues\", xticklabels=[\"Ham\", \"Spam\"], yticklabels=[\"Ham\", \"Spam\"])\n",
        "plt.xlabel(\"Vorhergesagt\")\n",
        "plt.ylabel(\"Tats√§chlich\")\n",
        "plt.title(\"üìä Confusion Matrix\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# ‚úÖ Save metrics\n",
        "bert_metrics = {\n",
        "    \"model\": model_name,\n",
        "    \"model_version\": f\"{model_name.replace('/', '_')}_v9\",\n",
        "    \"threshold\": 0.5,\n",
        "    \"precision\": precision_score(all_labels, all_preds),\n",
        "    \"recall\": recall_score(all_labels, all_preds),\n",
        "    \"f1_score\": f1_score(all_labels, all_preds),\n",
        "    \"accuracy\": (np.array(all_labels) == np.array(all_preds)).mean(),\n",
        "    \"config\": {\n",
        "        \"frozen\": False,\n",
        "        \"pretrained\": model_name,\n",
        "        \"epochs\": len(train_losses),\n",
        "        \"max_length\": 64,\n",
        "        \"batch_size\": 16\n",
        "    }\n",
        "}\n",
        "\n",
        "Path(\"results\").mkdir(exist_ok=True)\n",
        "metrics_path = f\"results/metrics_{model_name.replace('/', '_')}.json\"\n",
        "with open(metrics_path, \"w\") as f:\n",
        "    json.dump(bert_metrics, f, indent=4)\n",
        "\n",
        "from google.colab import files\n",
        "files.download(metrics_path)\n"
      ]
    }
  ]
}